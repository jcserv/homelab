namespace: monitoring

# Grafana Alloy configuration for log collection
alloy:
  alloy:
    # Clustering - disabled for simplicity
    clustering:
      enabled: false

    configMap:
      create: true
      content: |-
        logging {
          level  = "info"
          format = "logfmt"
        }

        discovery.kubernetes "pods" {
          role = "pod"

          // Only discover pods on this node to reduce CPU usage
          selectors {
            role  = "pod"
            field = "spec.nodeName=" + env("HOSTNAME")
          }
        }

        discovery.kubernetes "nodes" {
          role = "node"
        }

        // Relabel pod logs
        discovery.relabel "pod_logs" {
          targets = discovery.kubernetes.pods.targets

          // Only scrape running pods
          rule {
            source_labels = ["__meta_kubernetes_pod_phase"]
            action        = "keep"
            regex         = "Running"
          }

          // Drop monitoring and kube-system namespace logs (low value, high noise)
          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            action        = "drop"
            regex         = "monitoring|kube-system"
          }

          // Drop loki gateway access logs (noisy, creates unnecessary volume)
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            action        = "drop"
            regex         = "loki-gateway.*"
          }

          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            target_label  = "namespace"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            target_label  = "pod"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            target_label  = "container"
          }

          rule {
            source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_name"]
            separator     = "/"
            target_label  = "job"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_node_name"]
            target_label  = "node_name"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
            separator     = "/"
            target_label  = "__path__"
            replacement   = "/var/log/pods/*$1/*.log"
          }
        }

        // Scrape pod logs
        loki.source.kubernetes "pod_logs" {
          targets    = discovery.relabel.pod_logs.output
          forward_to = [loki.process.pod_logs.receiver]
        }

        // Process pod logs
        loki.process "pod_logs" {
          // Extract log level if present (case-insensitive)
          stage.regex {
            expression = "(?i)(?P<detected_level>debug|info|warn|error|fatal|panic)"
          }

          // Create normalized lowercase level field for consistent filtering
          stage.template {
            source   = "level"
            template = "{{`{{ ToLower .detected_level }}`}}"
          }

          stage.labels {
            values = {
              level = "",
            }
          }

          // Drop nginx-ingress successful requests (non-5XX) to reduce log volume
          // Only keep error logs (5XX status codes)
          stage.match {
            selector = "{namespace=\"ingress-nginx\"}"

            // Drop logs with status codes 1XX, 2XX, 3XX, 4XX
            // Nginx access log format: "METHOD /path HTTP/x.x" STATUS ...
            stage.drop {
              expression = "\"(GET|POST|PUT|DELETE|PATCH|HEAD|OPTIONS)\\s.*?HTTP/[0-9.]+\"\\s+[1-4][0-9]{2}\\s"
            }
          }

          forward_to = [loki.write.default.receiver]
        }

        // Collect systemd journal (system logs)
        loki.source.journal "system_logs" {
          path          = "/var/log/journal"
          forward_to    = [loki.process.system_logs.receiver]

          // Add labels to match pod log format
          labels = {
            job       = "systemd-journal",
            pod       = "systemd-journal",
            node_name = env("HOSTNAME"),
          }
        }

        // Process system logs
        loki.process "system_logs" {
          // Extract priority level and map to standard levels
          stage.labels {
            values = {
              priority = "",
            }
          }

          forward_to = [loki.write.default.receiver]
        }

        loki.write "default" {
          endpoint {
            url = "http://loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push"

            // Batching configuration - optimized for memory usage
            batch_wait = "15s"  // Increased to send less frequently, reduce memory churn
            batch_size = "2MiB"  // Larger batches, fewer sends

            // Queue configuration to limit memory usage
            queue_config {
              capacity = "2MiB"  // Further reduced to limit buffering
            }
          }

          external_labels = {
            cluster = "homelab",
          }

          // Limit number of active log streams to prevent unbounded memory growth
          max_streams = 2000  // Reduced from 3000 to save ~30% stream overhead
        }

  controller:
    type: daemonset

    # Host paths for reading pod logs and system journal
    volumes:
      extra:
        - name: varlog
          hostPath:
            path: /var/log
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
        - name: journal
          hostPath:
            path: /var/log/journal

    volumeMounts:
      extra:
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: journal
          mountPath: /var/log/journal
          readOnly: true

    # Tolerations to run on all nodes including control plane
    tolerations:
      - effect: NoSchedule
        operator: Exists

  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m # Increased to 500m to accommodate Pi4 workload with log collection
      memory: 2048Mi # Set to 2GB - alloy now only handles log collection (metrics handled by Prometheus directly)

  rbac:
    create: true

  serviceAccount:
    create: true

  service:
    enabled: true
    type: ClusterIP

  serviceMonitor:
    enabled: true
    additionalLabels:
      release: prometheus
    interval: 60s
