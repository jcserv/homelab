namespace: monitoring

alloy:
  alloy:
    # Clustering - disabled for simplicity
    clustering:
      enabled: false

    configMap:
      create: true
      content: |-
        logging {
          level  = "info"
          format = "logfmt"
        }

        discovery.kubernetes "pods" {
          role = "pod"

          // Only discover pods on this node to reduce CPU usage
          selectors {
            role  = "pod"
            field = "spec.nodeName=" + env("HOSTNAME")
          }
        }

        discovery.kubernetes "nodes" {
          role = "node"
        }

        // Relabel pod logs
        discovery.relabel "pod_logs" {
          targets = discovery.kubernetes.pods.targets

          // Only scrape running pods
          rule {
            source_labels = ["__meta_kubernetes_pod_phase"]
            action        = "keep"
            regex         = "Running"
          }

          // Drop monitoring and kube-system namespace logs (low value, high noise)
          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            action        = "drop"
            regex         = "monitoring|kube-system"
          }

          // Drop loki gateway access logs (noisy, creates unnecessary volume)
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            action        = "drop"
            regex         = "loki-gateway.*"
          }

          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            target_label  = "namespace"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            target_label  = "pod"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            target_label  = "container"
          }

          rule {
            source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_name"]
            separator     = "/"
            target_label  = "job"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_node_name"]
            target_label  = "node_name"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
            separator     = "/"
            target_label  = "__path__"
            replacement   = "/var/log/pods/*$1/*.log"
          }
        }

        // Scrape pod logs
        loki.source.kubernetes "pod_logs" {
          targets    = discovery.relabel.pod_logs.output
          forward_to = [loki.process.pod_logs.receiver]
        }

        // Process pod logs
        loki.process "pod_logs" {
          // Extract log level if present
          stage.regex {
            expression = "(?i)(?P<level>(debug|info|warn|error|fatal|panic))"
          }

          stage.labels {
            values = {
              level = "",
            }
          }

          forward_to = [loki.write.default.receiver]
        }

        loki.write "default" {
          endpoint {
            url = "http://loki-gateway.monitoring.svc.cluster.local/loki/api/v1/push"

            // Batching configuration - optimized for memory usage
            batch_wait = "15s"  // Increased to send less frequently, reduce memory churn
            batch_size = "2MiB"  // Larger batches, fewer sends

            // Queue configuration to limit memory usage
            queue_config {
              capacity = "2MiB"  // Further reduced to limit buffering
            }
          }

          external_labels = {
            cluster = "homelab",
          }

          // Limit number of active log streams to prevent unbounded memory growth
          max_streams = 2000  // Reduced from 3000 to save ~30% stream overhead
        }

        // Prometheus remote write for collected metrics
        prometheus.remote_write "default" {
          endpoint {
            url = "http://prometheus-kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090/api/v1/write"

            queue_config {
              capacity             = 2000  // Increased from 1000 to prevent saturation (was at 99.95%)
              max_shards           = 3     // Reduced from 5 to limit memory (3x2000 = 6000 max samples)
              min_shards           = 1
              max_samples_per_send = 500   // Keep at 500 to limit per-batch memory
              batch_send_deadline  = "5s"
              min_backoff          = "30ms"
              max_backoff          = "5s"
            }
          }
        }

        // Discover Prometheus service monitors
        // Scoped to specific namespaces to reduce cluster-wide API calls
        prometheus.operator.servicemonitors "default" {
          namespaces   = ["monitoring", "default"]  // Removed kube-system (all system ServiceMonitors are in monitoring namespace)
          forward_to   = [prometheus.remote_write.default.receiver]
        }

  controller:
    type: daemonset

    # Host paths for reading pod logs
    volumes:
      extra:
        - name: varlog
          hostPath:
            path: /var/log
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers

    volumeMounts:
      extra:
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true

    # Tolerations to run on all nodes including control plane
    tolerations:
      - effect: NoSchedule
        operator: Exists

  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      cpu: 500m # Increased to 500m to accommodate Pi4 workload with log collection
      memory: 2560Mi # Increased to 2.5GB based on actual usage (~2.7GB), with headroom for optimizations

  rbac:
    create: true

  serviceAccount:
    create: true

  service:
    enabled: true
    type: ClusterIP

  serviceMonitor:
    enabled: true
    additionalLabels:
      release: prometheus
    interval: 60s
