namespace: monitoring

kube-prometheus-stack:
  # Default rules for monitoring
  defaultRules:
    create: true
    rules:
      alertmanager: false
      etcd: false
      configReloaders: true
      general: true
      k8s: true
      kubeApiserverAvailability: true
      kubeApiserverBurnrate: true
      kubeApiserverHistogram: true
      kubeApiserverSlos: true
      kubeControllerManager: false
      kubelet: true
      kubeProxy: false
      kubePrometheusGeneral: true
      kubePrometheusNodeRecording: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      kubeSchedulerAlerting: false
      kubeSchedulerRecording: false
      kubeStateMetrics: true
      network: true
      node: true
      nodeExporterAlerting: true
      nodeExporterRecording: true
      prometheus: true
      prometheusOperator: true

  # Alertmanager - disabled for now
  alertmanager:
    enabled: false

  # Kubelet metrics
  kubelet:
    enabled: true
    namespace: kube-system

    # Reduce kubelet metric collection frequency
    serviceMonitor:
      # Additional metric relabelings for cadvisor to reduce cardinality
      cAdvisorMetricRelabelings:
        # Drop all container metrics for pause/POD containers (huge cardinality reduction)
        - sourceLabels: [container]
          regex: '^$'
          action: drop

        # Drop high-cardinality per-cpu metrics (keep aggregated cpu totals)
        - sourceLabels: [__name__]
          regex: 'container_cpu_cfs_(periods|throttled)_total'
          action: drop

        # Drop less useful container metrics
        - sourceLabels: [__name__]
          regex: 'container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|threads|file_descriptors|sockets|processes|ulimits_soft)'
          action: drop

        # Drop most container filesystem metrics (keep basic usage)
        - sourceLabels: [__name__]
          regex: 'container_fs_(reads.*|writes.*|io_.*|sector.*|inodes_total|inodes_free)'
          action: drop

        # Drop container spec metrics (static metadata we don't need)
        - sourceLabels: [__name__]
          regex: 'container_spec_.*'
          action: drop

        # Drop last_seen (not useful for monitoring)
        - sourceLabels: [__name__]
          regex: 'container_last_seen'
          action: drop

  # Grafana configuration
  grafana:
    enabled: true
    adminPassword: admin

    # Sidecar for dashboards
    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: "1"
        folder: /tmp/dashboards
        searchNamespace: ALL

    # Ingress configuration
    ingress:
      enabled: true
      ingressClassName: nginx
      annotations:
        cert-manager.io/cluster-issuer: homelab-ca-issuer
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
      hosts:
        - grafana.home
      tls:
        - hosts:
            - grafana.home
          secretName: wildcard-home-tls

    # Persistence
    persistence:
      enabled: true
      storageClassName: local-path
      size: 10Gi

    # Resources - optimized for memory usage
    resources:
      requests:
        cpu: 100m
        memory: 192Mi
      limits:
        cpu: 500m
        memory: 384Mi

    # Data sources - Prometheus is auto-configured
    additionalDataSources:
      - name: Loki
        type: loki
        url: http://loki-gateway.monitoring.svc.cluster.local
        access: proxy
        isDefault: false
        jsonData:
          maxLines: 1000

    # Topology spread for HA
    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: grafana

  # Prometheus Operator
  prometheusOperator:
    enabled: true

    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi

    # Topology spread for HA
    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: prometheus-operator

  # Prometheus instance
  prometheus:
    enabled: true

    prometheusSpec:
      # Retention - optimized for memory usage
      retention: 2d
      retentionSize: "8GB"

      # Scrape interval - reduce frequency to save memory
      scrapeInterval: 60s
      scrapeTimeout: 10s
      evaluationInterval: 60s

      # Resources - temporarily higher to handle WAL replay with old data
      # Will reduce once old high-cardinality data ages out
      resources:
        requests:
          cpu: 300m
          memory: 768Mi
        limits:
          cpu: 1000m
          memory: 1536Mi

      # Storage - reduced for 2d retention
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: local-path
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi

      # Service monitors selector - match all
      serviceMonitorSelectorNilUsesHelmValues: false
      serviceMonitorSelector: {}

      podMonitorSelectorNilUsesHelmValues: false
      podMonitorSelector: {}

      # Topology spread for HA
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: prometheus

      # Additional scrape configs for services without serviceMonitor
      additionalScrapeConfigs: []

  # API Server monitoring - reduce high-cardinality metrics
  kubeApiServer:
    enabled: true
    serviceMonitor:
      metricRelabelings:
        # Drop high-cardinality histogram buckets (keep _count and _sum)
        - sourceLabels: [__name__]
          regex: 'apiserver_request_duration_seconds_bucket|apiserver_request_sli_duration_seconds_bucket|apiserver_response_sizes_bucket|apiserver_request_body_size_bytes_bucket|apiserver_watch_events_sizes_bucket'
          action: drop
        # Drop watch cache metrics
        - sourceLabels: [__name__]
          regex: 'apiserver_watch_cache.*'
          action: drop
        # Drop admission controller histogram buckets
        - sourceLabels: [__name__]
          regex: 'apiserver_admission_.*_bucket'
          action: drop
        # Drop workqueue histogram buckets
        - sourceLabels: [__name__]
          regex: 'workqueue_.*_bucket'
          action: drop

  # Etcd monitoring - reduce high-cardinality metrics
  kubeEtcd:
    enabled: true
    serviceMonitor:
      metricRelabelings:
        # Drop high-cardinality histogram buckets
        - sourceLabels: [__name__]
          regex: 'etcd_request_duration_seconds_bucket|etcd_disk_wal_fsync_duration_seconds_bucket|etcd_disk_backend_commit_duration_seconds_bucket'
          action: drop

  # Kube State Metrics
  kube-state-metrics:
    enabled: true

    resources:
      requests:
        cpu: 10m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 128Mi

  # Node Exporter - collects hardware and OS metrics
  prometheus-node-exporter:
    enabled: true

    resources:
      requests:
        cpu: 100m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 128Mi

    # Run on all nodes including control plane
    hostNetwork: true
    hostPID: true
    tolerations:
      - effect: NoSchedule
        operator: Exists

    # ServiceMonitor configuration to prevent duplicate series during rolling updates
    prometheus:
      monitor:
        enabled: true
        relabelings:
          # Drop the pod label to prevent duplicates during rolling updates
          # We care about node metrics, not which pod collected them
          - action: labeldrop
            regex: pod
